{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1125b282-6f27-4510-be72-dbae13797e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: numba.jit seems to not be available. Using a dummy decorator for numba.jit() ...\n",
      "If you want the speed up brought by numba.jit, try to manually install numba and check that it works (installing llvmlite can be tricky, cf. https://github.com/numba/numba#custom-python-environments\n",
      "Info: Using the Jupyter notebook version of the tqdm() decorator, tqdm_notebook() ...\n",
      "ERROR: 'resource' module not available, but it is in the standard library.\n",
      "Have you messed up your Python installation?\n",
      "Are you on Windows? In this case, it's okay.\n",
      "Please submit a new bug on https://github.com/SMPyBandits/SMPyBandits/issues/new\n",
      "Warning: numba.jit seems to not be available. Using a dummy decorator for numba.jit() ...\n",
      "If you want the speed up brought by numba.jit, try to manually install numba and check that it works (installing llvmlite can be tricky, cf. https://github.com/numba/numba#custom-python-environments\n"
     ]
    }
   ],
   "source": [
    "import numpy.random as rn\n",
    "from random import random\n",
    "import numpy as np\n",
    "# Local imports\n",
    "from SMPyBandits.Environment import Evaluator, tqdm\n",
    "# Import arms\n",
    "from SMPyBandits.Arms import Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf27801-cd20-4a02-8ea8-fd9ac04bd7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import algorithms\n",
    "from SMPyBandits.Policies import EmpiricalMeans, EpsilonGreedy, UCB\n",
    "from SMPyBandits.Policies.IndexPolicy import IndexPolicy\n",
    "from SMPyBandits.Policies.BasePolicy import BasePolicy\n",
    "from SMPyBandits.Policies.with_proba import with_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831206c-ac23-4adf-b2ec-03bc82751f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EmpiricalMeans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a1420-f113-44a3-bd34-bfb90f408d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EmpiricalMeans??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032fcdbd-41e3-47d9-afea-8dd2dadc43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAB environment parameters\n",
    "HORIZON = 10000\n",
    "REPETITIONS = 100 # Number of repetition of the experiment (to have an average)\n",
    "N_JOBS = 1 # Number of CPU cores\n",
    "\n",
    "#: Default value for epsilon for `YourEpsilonGreedy`\n",
    "EPSILON = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e20b8f-ba82-49a5-945d-ed35b7a1d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS = [  # 1)  Bernoulli arms\n",
    "        {   # An easy but widely adopted problem\n",
    "            \"arm_type\": Bernoulli,\n",
    "            \"params\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        },\n",
    "        {   # An other problem, best arm = last, with three groups: very bad arms (0.01, 0.02), middle arms (0.3 - 0.6) and very good arms (0.78, 0.8, 0.82)\n",
    "            \"arm_type\": Bernoulli,\n",
    "            \"params\": [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.795, 0.8, 0.805]\n",
    "        },\n",
    "        {   # A very hard problem, as used in [Capp√© et al, 2012]\n",
    "            \"arm_type\": Bernoulli,\n",
    "            \"params\": [0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1]\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0740d6-6b39-4f68-9067-bbfe464ae779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class YourEpsilonGreedy(BasePolicy):\n",
    "    r\"\"\" The epsilon-greedy random policy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nbArms, epsilon=EPSILON, lower=0., amplitude=1.):\n",
    "        super(YourEpsilonGreedy, self).__init__(nbArms, lower=lower, amplitude=amplitude)\n",
    "        assert 0 <= epsilon <= 1, \"Error: the 'epsilon' parameter for YourEpsilonGreedy has to be in [0, 1].\"  # For DEBUG\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "\n",
    "    # This decorator @property makes this method an attribute, cf. https://docs.python.org/3/library/functions.html#property\n",
    "    @property\n",
    "    def epsilon(self):  # This allows us to use time-dependent epsilon coef\n",
    "        return self._epsilon\n",
    "\n",
    "    def choice(self):\n",
    "        \"\"\"With a probability of epsilon, explore (uniform choice), otherwhise exploit based on  empirical mean rewards.\"\"\"\n",
    "        #----------Your Code----------#\n",
    "\n",
    "        if random() < self.epsilon:\n",
    "            # Exploration: choose a random arm\n",
    "            return rn.randint(self.nbArms)\n",
    "        else:\n",
    "            # Exploitation: choose the best arm\n",
    "            return np.argmax(np.zeros(self.nbArms))\n",
    "        \n",
    "        \"\"\"\n",
    "        below are problem(b) (alpha thingy)'s choice function\n",
    "        \n",
    "        psilon_t = pow(self._t, -self._alpha)  # Calculate diminishing epsilon\n",
    "        self._t += 1  # Increment time step\n",
    "        if np.random.random() < epsilon_t:\n",
    "            # Exploration: Choose a random arm\n",
    "            return np.random.randint(self.nbArms)\n",
    "        else:\n",
    "            # Exploitation: Choose the best arm based on empirical rewards\n",
    "            return np.argmax(self.rewards)\n",
    "        \"\"\"\n",
    "\n",
    "        #----------End of Your Code----------#\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e9d85-d66e-4770-8627-f95575ec847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICIES = [\n",
    "        # --- EmpiricalMeans (or Greedy) algorithm, a baseline for comparison\n",
    "        {\n",
    "            \"archtype\": EmpiricalMeans,\n",
    "            \"params\": {}\n",
    "        },\n",
    "        # --- UCB algorithm, a baseline for comparison\n",
    "        {\n",
    "            \"archtype\": UCB,\n",
    "            \"params\": {}\n",
    "        },\n",
    "        # --- YourEpsilonGreedy algorithm\n",
    "        {\n",
    "            \"archtype\": YourEpsilonGreedy,\n",
    "            \"params\": {}\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change epsilon\n",
    "POLICIES = [\n",
    "    {\"archtype\": YourEpsilonGreedy, \"params\": {\"epsilon\": 0.01}},\n",
    "    {\"archtype\": YourEpsilonGreedy, \"params\": {\"epsilon\": 0.03}},\n",
    "    {\"archtype\": YourEpsilonGreedy, \"params\": {\"epsilon\": 0.1}},\n",
    "    {\"archtype\": YourEpsilonGreedy, \"params\": {\"epsilon\": 0.3}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c09d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change alpha\n",
    "ALPHA_VALUES = [0.1, 0.5, 1.0, 2.0]\n",
    "POLICIES = [\n",
    "    {\"archtype\": YourEpsilonGreedy, \"params\": {\"alpha\": alpha}} for alpha in ALPHA_VALUES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642164b3-aed0-4a29-8f07-e4af50afa2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    # --- Duration of the experiment\n",
    "    \"horizon\": HORIZON,\n",
    "    # --- Number of repetition of the experiment (to have an average)\n",
    "    \"repetitions\": REPETITIONS,\n",
    "    # --- Parameters for the use of joblib.Parallel\n",
    "    \"n_jobs\": N_JOBS,    # Number of CPU cores\n",
    "    \"verbosity\": 6,      # Max joblib verbosity\n",
    "    # --- Arms\n",
    "    \"environment\": ENVIRONMENTS,\n",
    "    # --- Algorithms\n",
    "    \"policies\": POLICIES,\n",
    "}\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375bc61-62cb-424e-b97e-4ad378b6e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Evaluator object\n",
    "evaluation = Evaluator(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94d023-8ca4-4dd1-8083-73039a678620",
   "metadata": {},
   "outputs": [],
   "source": [
    "for envId, env in tqdm(enumerate(evaluation.envs), desc=\"Problems\"):\n",
    "    # Evaluate just that env\n",
    "    evaluation.startOneEnv(envId, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec37ad-ff87-4f28-9569-cb1058a4f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAll(evaluation, envId):\n",
    "    evaluation.printFinalRanking(envId)\n",
    "    evaluation.plotRegrets(envId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c6927-3d39-4836-8c09-007f92dc72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "envId = 0\n",
    "plotAll(evaluation, envId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37971067-c448-485a-ab90-4800b8e238ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
